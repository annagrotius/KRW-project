{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "# nlp = spacy.load(r'C:\\\\Users\\\\Marta\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.2.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe from query results. (QUERY API: https://krr.triply.cc/annadg/-/queries/Abstract-Data-Query/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('entityQueryResults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply a pattern rule-based function that extract the sentence with a lemma of hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(text):\n",
    "    \"\"\"function to find sentences that contain the lemma of hypothesis\"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    # Add match ID \"HypothesisIs\" with no callback and one pattern\n",
    "    pattern = [{'LEMMA':{\"IN\":[\"hypothesis\",\"hypothesize\",\"hypothesise\", \"hypothesized\", \"hypothesised\"]}}]\n",
    "    \n",
    "    matcher.add(\"HypothesisIs\", None, pattern)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        sent = span.sent\n",
    "        return sent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pattern_match'] = data['value'].apply(pattern_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to get rid of this instance, otherwise the merge noun chunks does not work. Drop the index number returned here in the following cell. \n",
    "data[data['pattern_match']==\"While the hypothesis that dromedary camels are the likely major source of MERS-CoV infection in humans is gaining acceptance, conjecture continues over the original natural reservoir host(s)\"].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([data.index[399]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>article_types</th>\n",
       "      <th>new_id</th>\n",
       "      <th>pm_central_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>value</th>\n",
       "      <th>abstract_entities</th>\n",
       "      <th>pattern_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://ns.inria.fr/covid19/ceda3494dad0563197a...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/ceda349...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/ceda3494dad0563197a...</td>\n",
       "      <td>Summary Severe Acute Respiratory Syndrome (SAR...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/0ff833...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>http://ns.inria.fr/covid19/bdc91980735543fbf8c...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/bdc9198...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/bdc91980735543fbf8c...</td>\n",
       "      <td>Abstract For epidemic control, rapid identific...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/0c5288...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>http://ns.inria.fr/covid19/5d53384b0c11ff23069...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/PMC7087...</td>\n",
       "      <td>PMC7087228</td>\n",
       "      <td>http://ns.inria.fr/covid19/5d53384b0c11ff23069...</td>\n",
       "      <td>Trionyx sinensis hemorrhagic syndrome virus (T...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/0cf5ee...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>http://ns.inria.fr/covid19/40a07bdce5fdd3382c1...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/40a07bd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/40a07bdce5fdd3382c1...</td>\n",
       "      <td>Abstract This study adopted a two (author: alg...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/0bcce4...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>http://ns.inria.fr/covid19/f8f464606fc9d44a2cd...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/f8f4646...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/f8f464606fc9d44a2cd...</td>\n",
       "      <td>Abstract Objective To evaluate the health effe...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/78f8b4...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>http://ns.inria.fr/covid19/f4e399086421d7a4ee5...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/f4e3990...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/f4e399086421d7a4ee5...</td>\n",
       "      <td>Abstract Suckling C57BU6 mice infected with mo...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/370225...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>http://ns.inria.fr/covid19/704854881dfad026bde...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/7048548...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/704854881dfad026bde...</td>\n",
       "      <td>Background The risk of developing nosocomial i...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/1a1b7e...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>http://ns.inria.fr/covid19/0c1b5e06659621ae245...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/PMC7108...</td>\n",
       "      <td>PMC7108410</td>\n",
       "      <td>http://ns.inria.fr/covid19/0c1b5e06659621ae245...</td>\n",
       "      <td>Abstract A new personal bioaerosol sampler has...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/1a842c...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>http://ns.inria.fr/covid19/01e3d931c0bc070ec62...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/PMC5583...</td>\n",
       "      <td>PMC5583561</td>\n",
       "      <td>http://ns.inria.fr/covid19/01e3d931c0bc070ec62...</td>\n",
       "      <td>Medical countermeasures, including new drugs a...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/16156c...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>http://ns.inria.fr/covid19/5558b9b15a0a761b7e4...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/5558b9b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/5558b9b15a0a761b7e4...</td>\n",
       "      <td>Résumé Nous présentons 2 cas de maladie de Kaw...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/400fe0...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paper  \\\n",
       "8    http://ns.inria.fr/covid19/ceda3494dad0563197a...   \n",
       "30   http://ns.inria.fr/covid19/bdc91980735543fbf8c...   \n",
       "32   http://ns.inria.fr/covid19/5d53384b0c11ff23069...   \n",
       "50   http://ns.inria.fr/covid19/40a07bdce5fdd3382c1...   \n",
       "53   http://ns.inria.fr/covid19/f8f464606fc9d44a2cd...   \n",
       "..                                                 ...   \n",
       "881  http://ns.inria.fr/covid19/f4e399086421d7a4ee5...   \n",
       "893  http://ns.inria.fr/covid19/704854881dfad026bde...   \n",
       "909  http://ns.inria.fr/covid19/0c1b5e06659621ae245...   \n",
       "913  http://ns.inria.fr/covid19/01e3d931c0bc070ec62...   \n",
       "916  http://ns.inria.fr/covid19/5558b9b15a0a761b7e4...   \n",
       "\n",
       "                                         article_types  \\\n",
       "8    http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "30   http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "32   http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "50   http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "53   http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "..                                                 ...   \n",
       "881  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "893  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "909  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "913  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "916  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "\n",
       "                                                new_id pm_central_id  \\\n",
       "8    http://example.org/hypothesis_ontology/ceda349...           NaN   \n",
       "30   http://example.org/hypothesis_ontology/bdc9198...           NaN   \n",
       "32   http://example.org/hypothesis_ontology/PMC7087...    PMC7087228   \n",
       "50   http://example.org/hypothesis_ontology/40a07bd...           NaN   \n",
       "53   http://example.org/hypothesis_ontology/f8f4646...           NaN   \n",
       "..                                                 ...           ...   \n",
       "881  http://example.org/hypothesis_ontology/f4e3990...           NaN   \n",
       "893  http://example.org/hypothesis_ontology/7048548...           NaN   \n",
       "909  http://example.org/hypothesis_ontology/PMC7108...    PMC7108410   \n",
       "913  http://example.org/hypothesis_ontology/PMC5583...    PMC5583561   \n",
       "916  http://example.org/hypothesis_ontology/5558b9b...           NaN   \n",
       "\n",
       "                                              abstract  \\\n",
       "8    http://ns.inria.fr/covid19/ceda3494dad0563197a...   \n",
       "30   http://ns.inria.fr/covid19/bdc91980735543fbf8c...   \n",
       "32   http://ns.inria.fr/covid19/5d53384b0c11ff23069...   \n",
       "50   http://ns.inria.fr/covid19/40a07bdce5fdd3382c1...   \n",
       "53   http://ns.inria.fr/covid19/f8f464606fc9d44a2cd...   \n",
       "..                                                 ...   \n",
       "881  http://ns.inria.fr/covid19/f4e399086421d7a4ee5...   \n",
       "893  http://ns.inria.fr/covid19/704854881dfad026bde...   \n",
       "909  http://ns.inria.fr/covid19/0c1b5e06659621ae245...   \n",
       "913  http://ns.inria.fr/covid19/01e3d931c0bc070ec62...   \n",
       "916  http://ns.inria.fr/covid19/5558b9b15a0a761b7e4...   \n",
       "\n",
       "                                                 value  \\\n",
       "8    Summary Severe Acute Respiratory Syndrome (SAR...   \n",
       "30   Abstract For epidemic control, rapid identific...   \n",
       "32   Trionyx sinensis hemorrhagic syndrome virus (T...   \n",
       "50   Abstract This study adopted a two (author: alg...   \n",
       "53   Abstract Objective To evaluate the health effe...   \n",
       "..                                                 ...   \n",
       "881  Abstract Suckling C57BU6 mice infected with mo...   \n",
       "893  Background The risk of developing nosocomial i...   \n",
       "909  Abstract A new personal bioaerosol sampler has...   \n",
       "913  Medical countermeasures, including new drugs a...   \n",
       "916  Résumé Nous présentons 2 cas de maladie de Kaw...   \n",
       "\n",
       "                                     abstract_entities pattern_match  \n",
       "8    https://krr.triply.cc/.well-known/genid/0ff833...          None  \n",
       "30   https://krr.triply.cc/.well-known/genid/0c5288...          None  \n",
       "32   https://krr.triply.cc/.well-known/genid/0cf5ee...          None  \n",
       "50   https://krr.triply.cc/.well-known/genid/0bcce4...          None  \n",
       "53   https://krr.triply.cc/.well-known/genid/78f8b4...          None  \n",
       "..                                                 ...           ...  \n",
       "881  https://krr.triply.cc/.well-known/genid/370225...          None  \n",
       "893  https://krr.triply.cc/.well-known/genid/1a1b7e...          None  \n",
       "909  https://krr.triply.cc/.well-known/genid/1a842c...          None  \n",
       "913  https://krr.triply.cc/.well-known/genid/16156c...          None  \n",
       "916  https://krr.triply.cc/.well-known/genid/400fe0...          None  \n",
       "\n",
       "[92 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating bool series True for NaN values - as the subsequent formula will break if there are \n",
    "bool_series = pd.isnull(data[\"pattern_match\"])  \n",
    "    \n",
    "# filtering data  \n",
    "data[bool_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the predominant aetiological agent of the common cold, human rhinovirus (HRV) is the leading cause of human infectious disease. Early studies showed that a monovalent formalin-inactivated HRV vaccine can be protective, and virus-neutralizing antibodies (nAb) correlated with protection. However, co-circulation of many HRV types discouraged further vaccine efforts. Here, we test the hypothesis that increasing virus input titres in polyvalent inactivated HRV vaccine may result in broad nAb responses. We show that serum nAb against many rhinovirus types can be induced by polyvalent, inactivated HRVs plus alhydrogel (alum) adjuvant. Using formulations up to 25-valent in mice and 50-valent in rhesus macaques, HRV vaccine immunogenicity was related to sufficient quantity of input antigens, and valency was not a major factor for potency or breadth of the response. Thus, we have generated a vaccine capable of inducing nAb responses to numerous and diverse HRV types. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate abstracts as not matching lemma pattern to verify integrity of pattern\n",
    "\n",
    "for row in data.value[881:882]:  # iterating through the rows of the object column\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'pattern match' does not return a match\n",
    "data.dropna(subset=['pattern_match'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract hypothesis entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "\n",
    "def merge_noun_chunks(text):\n",
    "    \"\"\"function to merge noun chunks in texts\"\"\"\n",
    "    noun_chunks = []\n",
    "    for t in nlp(text):\n",
    "        noun_chunks.append(t.text)\n",
    "        \n",
    "    return noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['merged_noun_chunks'] = data['pattern_match'].apply(merge_noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [Based, on, this hypothesis, ,, a T-cell vacci...\n",
       "1      [However, ,, enhancements, made, to, EDNA, inc...\n",
       "2      [In, a follow up study, ,, we, hypothesized, t...\n",
       "3      [We, hypothesized, that, a combinatorial prote...\n",
       "4      [We, hypothesize, that, SES, is, caused, by, a...\n",
       "                             ...                        \n",
       "926    [Molecular tools, enable, one, to, trace, the ...\n",
       "927    [We, previously, hypothesized, that, HIV-1, co...\n",
       "928    [We, hypothesised, that, healthy children, wit...\n",
       "929    [We, hypothesized, that, this change, would, a...\n",
       "930    [While, there, are, several hypotheses, that, ...\n",
       "Name: merged_noun_chunks, Length: 838, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['merged_noun_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(list_of_chunks):\n",
    "    \"\"\"\n",
    "    Input: list of strings \n",
    "    Output: new list of strings containing only the strings that were >1 word; the returned strings are connected by '_'  \n",
    "    \"\"\"\n",
    "    for index, word in enumerate(list_of_chunks):\n",
    "        if len(word.split(' ')) > 1:\n",
    "            new_word = word.replace(' ', '_')\n",
    "            list_of_chunks[index] = new_word\n",
    "    sentence = ' '.join(list_of_chunks)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "data['merged_sent'] = data['merged_noun_chunks'].apply(combine_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution from https://stackoverflow.com/questions/59993683/how-can-i-get-spacy-to-stop-splitting-both-hyphenated-numbers-and-words-into-sep \n",
    "def custom_tokenizer(nlp):\n",
    "    \"\"\"\n",
    "    Function that keeps intra-hyphenated words as single tokens.\n",
    "    \"\"\"\n",
    "    inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=nlp.Defaults.tokenizer_exceptions)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(hypothesis_sentence):\n",
    "    \"\"\"\n",
    "    Takes a sentence as input and tokenizes it. Outputs the words that have the certain POS label. \n",
    "    \"\"\"    \n",
    "    hypothesis_keywords = []\n",
    "    doc = nlp(hypothesis_sentence)\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"PROPN\" or tok.pos_ == \"NOUN\":\n",
    "            hypothesis_keywords.append(tok.text)\n",
    "   \n",
    "    return hypothesis_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hypothesis_entities'] = data['merged_sent'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data into dataframe with desired columns for future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = ['abstract_entities', 'merged_noun_chunks', 'merged_sent'] , inplace=True)\n",
    "data.rename(columns={\"pattern_match\":\"hypothesis_sentence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hypothesis_entities(text):\n",
    "    \"\"\"\n",
    "    Cleans the '_' from the sentences created in previous functions.\n",
    "    \"\"\"\n",
    "    cleaned_hypotheses = []\n",
    "    for word in (text):\n",
    "        word.split(' ')\n",
    "        new_word = word.replace('_', ' ')\n",
    "        cleaned_hypotheses.append(new_word)\n",
    "    \n",
    "    return cleaned_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_hypothesis_entities\"] = data[\"hypothesis_entities\"].apply(clean_hypothesis_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 'entity_{}'.format(x + 1)\n",
    "entity_df = pd.DataFrame(data.clean_hypothesis_entities.values.tolist(),data.index, dtype=object).fillna('').rename(columns=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "entity_df = entity_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = (pd.concat([data,entity_df],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('paper_hyp_entity_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
