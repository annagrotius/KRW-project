{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "# nlp = spacy.load(r'C:\\\\Users\\\\Marta\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.2.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe from query results. (QUERY API: https://krr.triply.cc/annadg/-/queries/Abstract-Data-Query/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('entityQueryResults.csv')\n",
    "data = pd.read_csv('../sparql-queries/abstract-info-instances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply a pattern rule-based function that extract the sentence with a lemma of hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(text):\n",
    "    \"\"\"function to find sentences that contain the lemma of hypothesis\"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    # Add match ID \"HypothesisIs\" with no callback and one pattern\n",
    "    pattern = [{'LEMMA':{\"IN\":[\"hypothesis\",\"hypothesize\",\"hypothesise\", \"hypothesized\", \"hypothesised\"]}}]\n",
    "    \n",
    "    matcher.add(\"HypothesisIs\", None, pattern)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        sent = span.sent\n",
    "        return sent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pattern_match'] = data['value'].apply(pattern_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1173"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to get rid of this instance, otherwise the merge noun chunks does not work. Drop the index number returned here in the following cell. \n",
    "data[data['pattern_match']==\"While the hypothesis that dromedary camels are the likely major source of MERS-CoV infection in humans is gaining acceptance, conjecture continues over the original natural reservoir host(s)\"].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([data.index[1173]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>article_types</th>\n",
       "      <th>new_id</th>\n",
       "      <th>pm_central_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>value</th>\n",
       "      <th>abstract_entities</th>\n",
       "      <th>pattern_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://ns.inria.fr/covid19/1eb3f3f0aafd8b2741a...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/1eb3f3f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/1eb3f3f0aafd8b2741a...</td>\n",
       "      <td>Summary Objectives To measure the spatial cont...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/1683bd...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>http://ns.inria.fr/covid19/PMC4747015</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/PMC4747...</td>\n",
       "      <td>PMC4747015</td>\n",
       "      <td>http://ns.inria.fr/covid19/PMC4747015#abstract</td>\n",
       "      <td>Information regarding effective anesthetic reg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>http://ns.inria.fr/covid19/4b278bf04e245866207...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/4b278bf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/4b278bf04e245866207...</td>\n",
       "      <td>Abstract Global re-emergence of Chikungunya vi...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/0e1b11...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>http://ns.inria.fr/covid19/84317229254324b8c2d...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/PMC7106...</td>\n",
       "      <td>PMC7106413</td>\n",
       "      <td>http://ns.inria.fr/covid19/84317229254324b8c2d...</td>\n",
       "      <td>Summary The Coalition for Epidemic Preparednes...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/0000a5...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>http://ns.inria.fr/covid19/700cf45c09aadb04452...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/700cf45...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/700cf45c09aadb04452...</td>\n",
       "      <td>As of March 23, 2020 there have been over 354,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>http://ns.inria.fr/covid19/7461fe0adbb9a865f8a...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/PMC5661...</td>\n",
       "      <td>PMC5661933</td>\n",
       "      <td>http://ns.inria.fr/covid19/7461fe0adbb9a865f8a...</td>\n",
       "      <td>BACKGROUND: The detection of wild poliovirus i...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/027f07...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>http://ns.inria.fr/covid19/93df1925c1aa0cf7e72...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/PMC3282...</td>\n",
       "      <td>PMC3282974</td>\n",
       "      <td>http://ns.inria.fr/covid19/93df1925c1aa0cf7e72...</td>\n",
       "      <td>Kawasaki disease (KD) is a self-limited system...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>http://ns.inria.fr/covid19/PMC4187631</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/PMC4187...</td>\n",
       "      <td>PMC4187631</td>\n",
       "      <td>http://ns.inria.fr/covid19/PMC4187631#abstract</td>\n",
       "      <td>SUMMARY: The pathogenicity and clinical pertin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>http://ns.inria.fr/covid19/8d07b32de2cd9460999...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/8d07b32...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/8d07b32de2cd9460999...</td>\n",
       "      <td>A SEIR simulation model for the COVID-19 pande...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/04dd52...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>http://ns.inria.fr/covid19/f4e399086421d7a4ee5...</td>\n",
       "      <td>http://purl.org/ontology/bibo/AcademicArticle,...</td>\n",
       "      <td>http://example.org/hypothesis_ontology/f4e3990...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://ns.inria.fr/covid19/f4e399086421d7a4ee5...</td>\n",
       "      <td>Abstract Suckling C57BU6 mice infected with mo...</td>\n",
       "      <td>https://krr.triply.cc/.well-known/genid/370225...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  paper  \\\n",
       "7     http://ns.inria.fr/covid19/1eb3f3f0aafd8b2741a...   \n",
       "19                http://ns.inria.fr/covid19/PMC4747015   \n",
       "45    http://ns.inria.fr/covid19/4b278bf04e245866207...   \n",
       "52    http://ns.inria.fr/covid19/84317229254324b8c2d...   \n",
       "54    http://ns.inria.fr/covid19/700cf45c09aadb04452...   \n",
       "...                                                 ...   \n",
       "1181  http://ns.inria.fr/covid19/7461fe0adbb9a865f8a...   \n",
       "1188  http://ns.inria.fr/covid19/93df1925c1aa0cf7e72...   \n",
       "1212              http://ns.inria.fr/covid19/PMC4187631   \n",
       "1213  http://ns.inria.fr/covid19/8d07b32de2cd9460999...   \n",
       "1217  http://ns.inria.fr/covid19/f4e399086421d7a4ee5...   \n",
       "\n",
       "                                          article_types  \\\n",
       "7     http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "19    http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "45    http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "52    http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "54    http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "...                                                 ...   \n",
       "1181  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "1188  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "1212  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "1213  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "1217  http://purl.org/ontology/bibo/AcademicArticle,...   \n",
       "\n",
       "                                                 new_id pm_central_id  \\\n",
       "7     http://example.org/hypothesis_ontology/1eb3f3f...           NaN   \n",
       "19    http://example.org/hypothesis_ontology/PMC4747...    PMC4747015   \n",
       "45    http://example.org/hypothesis_ontology/4b278bf...           NaN   \n",
       "52    http://example.org/hypothesis_ontology/PMC7106...    PMC7106413   \n",
       "54    http://example.org/hypothesis_ontology/700cf45...           NaN   \n",
       "...                                                 ...           ...   \n",
       "1181  http://example.org/hypothesis_ontology/PMC5661...    PMC5661933   \n",
       "1188  http://example.org/hypothesis_ontology/PMC3282...    PMC3282974   \n",
       "1212  http://example.org/hypothesis_ontology/PMC4187...    PMC4187631   \n",
       "1213  http://example.org/hypothesis_ontology/8d07b32...           NaN   \n",
       "1217  http://example.org/hypothesis_ontology/f4e3990...           NaN   \n",
       "\n",
       "                                               abstract  \\\n",
       "7     http://ns.inria.fr/covid19/1eb3f3f0aafd8b2741a...   \n",
       "19       http://ns.inria.fr/covid19/PMC4747015#abstract   \n",
       "45    http://ns.inria.fr/covid19/4b278bf04e245866207...   \n",
       "52    http://ns.inria.fr/covid19/84317229254324b8c2d...   \n",
       "54    http://ns.inria.fr/covid19/700cf45c09aadb04452...   \n",
       "...                                                 ...   \n",
       "1181  http://ns.inria.fr/covid19/7461fe0adbb9a865f8a...   \n",
       "1188  http://ns.inria.fr/covid19/93df1925c1aa0cf7e72...   \n",
       "1212     http://ns.inria.fr/covid19/PMC4187631#abstract   \n",
       "1213  http://ns.inria.fr/covid19/8d07b32de2cd9460999...   \n",
       "1217  http://ns.inria.fr/covid19/f4e399086421d7a4ee5...   \n",
       "\n",
       "                                                  value  \\\n",
       "7     Summary Objectives To measure the spatial cont...   \n",
       "19    Information regarding effective anesthetic reg...   \n",
       "45    Abstract Global re-emergence of Chikungunya vi...   \n",
       "52    Summary The Coalition for Epidemic Preparednes...   \n",
       "54    As of March 23, 2020 there have been over 354,...   \n",
       "...                                                 ...   \n",
       "1181  BACKGROUND: The detection of wild poliovirus i...   \n",
       "1188  Kawasaki disease (KD) is a self-limited system...   \n",
       "1212  SUMMARY: The pathogenicity and clinical pertin...   \n",
       "1213  A SEIR simulation model for the COVID-19 pande...   \n",
       "1217  Abstract Suckling C57BU6 mice infected with mo...   \n",
       "\n",
       "                                      abstract_entities pattern_match  \n",
       "7     https://krr.triply.cc/.well-known/genid/1683bd...          None  \n",
       "19                                                  NaN          None  \n",
       "45    https://krr.triply.cc/.well-known/genid/0e1b11...          None  \n",
       "52    https://krr.triply.cc/.well-known/genid/0000a5...          None  \n",
       "54                                                  NaN          None  \n",
       "...                                                 ...           ...  \n",
       "1181  https://krr.triply.cc/.well-known/genid/027f07...          None  \n",
       "1188                                                NaN          None  \n",
       "1212                                                NaN          None  \n",
       "1213  https://krr.triply.cc/.well-known/genid/04dd52...          None  \n",
       "1217  https://krr.triply.cc/.well-known/genid/370225...          None  \n",
       "\n",
       "[116 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating bool series True for NaN values - as the subsequent formula will break if there are \n",
    "bool_series = pd.isnull(data[\"pattern_match\"])  \n",
    "    \n",
    "# filtering data  \n",
    "data[bool_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTIVES—To explore the hypothesis that hydrocarbon species and other air pollutants which accumulate at low and high concentrations of ozone are more directly associated with childhood wheezy episodes than ozone. METHODS—Prospective observational study over 1 year set in the Lewisham district of south east London. The daily attendance rate of children with acute wheeze at the accident and emergency department of Lewisham Hospital was related to local measurements of ozone, hydrocarbon species, nitrogen dioxide (NO(2)), sulphur dioxide (SO(2)) and small particulate matter with diameter <10µm (PM(10)). RESULTS—An inverse relation was found between the air pollutants and ozone. After seasonal and meteorological adjustment a non-linear U shaped trend was found between incidence of wheeze and ozone. The trend was significant in children <2 years of age but not in older children. In the younger age group, after adjustment for season, temperature, wind speed, and respiratory infection, the incidence relative to that at the mean daily ozone concentration of 32.7 µg/m(3), was estimated to increase by 65% (95% confidence interval (95% CI) 22% to 122%) at an ozone concentration of 5 µg/m(3) (1.5 SDs below the mean) and by 63% (95%CI −6% to 184%) at 80( )µg/m(3) (2.5 SDs above the mean). For several hydrocarbons there were significant positive linear relations found, again in children <2 years of age but not older children. For benzene, the incidence increased by 8% (95% CI 2 to 13%) per SD (SD 2.8 µg/m(3)) increase in benzene concentration. A same day association between incidence and ozone was found to be the most significant but for other pollutants a lag of 2 days gave the most significant associations. No significant association was found for the non-hydrocarbon pollutants including SO(2), NO(2), and PM(10). CONCLUSIONS—A U shaped relation was found between ozone and the incidence of wheezy episodes in young children. Certain hydrocarbon pollutants accumulate in the atmosphere when ozone concentrations are low, and are associated with childhood wheezy episodes. However, the U shaped association of ozone on incidence cannot be explained by these other pollutants. The finding supports an earlier finding that incidences of wheeze are at a minimum when ozone concentrations are 30-40 µg/m(3). Keywords: air pollution; ozone; hydrocarbons; childhood wheezing \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate abstracts as not matching lemma pattern to verify integrity of pattern\n",
    "\n",
    "for row in data.value[881:882]:  # iterating through the rows of the object column\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'pattern match' does not return a match\n",
    "data.dropna(subset=['pattern_match'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract hypothesis entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "\n",
    "def merge_noun_chunks(text):\n",
    "    \"\"\"function to merge noun chunks in texts\"\"\"\n",
    "    noun_chunks = []\n",
    "    for t in nlp(text):\n",
    "        noun_chunks.append(t.text)\n",
    "        \n",
    "    return noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['merged_noun_chunks'] = data['pattern_match'].apply(merge_noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(list_of_chunks):\n",
    "    \"\"\"\n",
    "    Input: list of strings \n",
    "    Output: new list of strings containing only the strings that were >1 word; the returned strings are connected by '_'  \n",
    "    \"\"\"\n",
    "    for index, word in enumerate(list_of_chunks):\n",
    "        if len(word.split(' ')) > 1:\n",
    "            new_word = word.replace(' ', '_')\n",
    "            list_of_chunks[index] = new_word\n",
    "    sentence = ' '.join(list_of_chunks)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "data['merged_sent'] = data['merged_noun_chunks'].apply(combine_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution from https://stackoverflow.com/questions/59993683/how-can-i-get-spacy-to-stop-splitting-both-hyphenated-numbers-and-words-into-sep \n",
    "def custom_tokenizer(nlp):\n",
    "    \"\"\"\n",
    "    Function that keeps intra-hyphenated words as single tokens.\n",
    "    \"\"\"\n",
    "    inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=nlp.Defaults.tokenizer_exceptions)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(hypothesis_sentence):\n",
    "    \"\"\"\n",
    "    Takes a sentence as input and tokenizes it. Outputs the words that have the certain POS label. \n",
    "    \"\"\"    \n",
    "    hypothesis_keywords = []\n",
    "    doc = nlp(hypothesis_sentence)\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"PROPN\" or tok.pos_ == \"NOUN\":\n",
    "            hypothesis_keywords.append(tok.text)\n",
    "   \n",
    "    return hypothesis_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hypothesis_entities'] = data['merged_sent'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data into dataframe with desired columns for future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = ['abstract_entities', 'merged_noun_chunks', 'merged_sent'] , inplace=True)\n",
    "data.rename(columns={\"pattern_match\":\"hypothesis_sentence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hypothesis_entities(text):\n",
    "    \"\"\"\n",
    "    Cleans the '_' from the sentences created in previous functions.\n",
    "    \"\"\"\n",
    "    cleaned_hypotheses = []\n",
    "    for word in (text):\n",
    "        word.split(' ')\n",
    "        new_word = word.replace('_', ' ')\n",
    "        cleaned_hypotheses.append(new_word)\n",
    "    \n",
    "    return cleaned_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_hypothesis_entities\"] = data[\"hypothesis_entities\"].apply(clean_hypothesis_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 'entity_{}'.format(x + 1)\n",
    "entity_df = pd.DataFrame(data.clean_hypothesis_entities.values.tolist(),data.index, dtype=object).fillna('').rename(columns=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "entity_df = entity_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = (pd.concat([data,entity_df],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('paper_hyp_entity_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
