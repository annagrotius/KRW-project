{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating SPARQL query to find useful MeSH concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create a SPARQL query to run in MeSH dataset. We want to find essential/useful MeSH concepts that might be connected with keywords found in our hypotheses. In order to do that we extracted previously merged_noun_chunks and keywords representing nouns in these hypotheses. Then we tokenize here the keywords and use them in regex-shaped query to find any MeSH concepts that have labels connected with these tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load essential libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the file with data from abstracts. The format of the file is csv (table) with columns: object (abstract), pattern_match (sentence with \"hypoth\"), merged_noun_chunks (from the sentence), merged_sent, keywords. We are interested only in keywords column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('abstract_data.csv')\n",
    "keywords = data.keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare our keywords data to tokenize the keywords from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_items = []\n",
    "for i in keywords:\n",
    "    i = i.replace('[', '')   \n",
    "    i = i.replace(']', '')  \n",
    "    clean_items.append(i)\n",
    "    \n",
    "data.keywords = clean_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(series):\n",
    "    clean_words = []\n",
    "    list_of_words = series.split(',')\n",
    "    for word in list_of_words:\n",
    "#word = word.replace('_', ' ')      \n",
    "        word = word.replace(\"'\", '')\n",
    "        word = word.lower()\n",
    "        word = word.strip(' ')\n",
    "        clean_words.append(word)\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keywords = data.keywords.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords = the noun entities taken from the merged noun_chunks\n",
    "Here we clean them and then tokenize them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_keyword(text):\n",
    "    cleaned_keywords = []\n",
    "    for word in (text):\n",
    "        word.split(' ')\n",
    "        new_word = word.replace('_', ' ')\n",
    "        cleaned_keywords.append(new_word)\n",
    "    \n",
    "    return cleaned_keywords\n",
    "\n",
    "def tokenize(text):\n",
    "    res = [sub.split() for sub in text]\n",
    "    flattened = [i for j in res for i in j]\n",
    "    return flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords_clean'] = data[\"keywords\"].astype(str)\n",
    "# clean the keywords\n",
    "data['keywords_clean'] = data['keywords'].apply(clean_keyword)\n",
    "# tokenize \n",
    "data['tokens'] = data['keywords_clean'].apply(tokenize)\n",
    "# put tokens into set\n",
    "data['tuple_tokens'] = data['tokens'].apply(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with keywords -- which are nouns that were extracted from noun-chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data.keywords[0:5]:  # iterating through the rows of the object column\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we tokenize these keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data.tokens[0:5]:  # iterating through the rows of the object column\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the functions that are run on the tokens sets to clean up the tokens column. We require \"datasets_freq_words.csv\" for these to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_double_char(ents):\n",
    "    \"\"\"Drop any entities that are less than three characters. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    ents -- a set of entities\n",
    "    \n",
    "    \"\"\"\n",
    "    drop_ents = {ent for ent in ents if len(ent) < 3}\n",
    "    return ents - drop_ents\n",
    "\n",
    "def keep_alpha(ents):\n",
    "    \"\"\"Keep only entities with alphabetical unicode characters, hyphens, and spaces. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    ents -- a set of entities\n",
    "    \n",
    "    \"\"\"\n",
    "    keep_char = set('-abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "    drop_ents = {ent for ent in ents if not set(ent).issubset(keep_char)}\n",
    "    return ents - drop_ents\n",
    "\n",
    "def drop_single_char_nps(ents):\n",
    "    \"\"\"Within an entity, drop single characters. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    ents -- a set of entities\n",
    "    \n",
    "    \"\"\"\n",
    "    return {' '.join([e for e in ent.split(' ') if not len(e) == 1]) for ent in ents}\n",
    "\n",
    "def remove_freq_words(entities):\n",
    "    \"\"\"Drop any entities in the 5000 most common words in the English langauge. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    ents -- a set of entities\n",
    "    \n",
    "    \"\"\"\n",
    "    freq_words = pd.read_csv('datasets_freq_words.csv')['Word'].iloc[1:]\n",
    "    for word in freq_words:\n",
    "        try:\n",
    "            entities.remove(word)\n",
    "        except KeyError:\n",
    "            continue # ignore the stop word if it's not in the list of abstract entities\n",
    "    return entities\n",
    "\n",
    "def add_clean_ents(df, funcs=[]):\n",
    "    \"\"\"Create new column in data frame with cleaned entities.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    df -- a dataframe object\n",
    "    funcs -- a list of heuristic functions to be applied to entities\n",
    "    \n",
    "    \"\"\"\n",
    "    col = 'tuple_tokens_clean'\n",
    "    df[col] = df['tuple_tokens']\n",
    "    for f in funcs:\n",
    "        df[col] = df[col].apply(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run all the functions through 'add clean ents function'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [drop_double_char, keep_alpha, drop_single_char_nps, remove_freq_words]\n",
    "add_clean_ents(data, functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the cleaned tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data.tuple_tokens_clean[0:5]:  # iterating through the rows of the object column\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the set of sets into a list, expand the list and create one final clean set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_list(text):\n",
    "    large_list = []\n",
    "    for word in (text):\n",
    "        word.split(',')\n",
    "        if word not in large_list:\n",
    "            large_list.append(word)\n",
    "    return large_list\n",
    "\n",
    "data[\"list_clean\"] = data[\"tuple_tokens_clean\"].apply(large_list)\n",
    "aggregated_list = data.list_clean.sum()\n",
    "\n",
    "unique_tokens = set()\n",
    "for word in aggregated_list:\n",
    "    unique_tokens.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the flattened set that we use to create the SPARQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define function to create the SPARQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparql_query(text):\n",
    "\n",
    "    print (\"WHERE {\")\n",
    "    print (\"?sub meshv:preferredConcept ?pa .\")\n",
    "    print (\"?pa rdfs:label ?paLabel .\")\n",
    "    print (\"FILTER(\")\n",
    "    for keyword in text:\n",
    "        print (f\"REGEX(?paLabel, \\'^{keyword}', 'i') ||\")\n",
    "    print (\"}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_query(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
